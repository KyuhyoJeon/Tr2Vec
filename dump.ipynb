{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_UCR(dataset):\n",
    "    train_file = os.path.join('Data/UCR', dataset, dataset + \"_TRAIN.tsv\")\n",
    "    test_file = os.path.join('Data/UCR', dataset, dataset + \"_TEST.tsv\")\n",
    "    train_df = pd.read_csv(train_file, sep='\\t', header=None)\n",
    "    test_df = pd.read_csv(test_file, sep='\\t', header=None)\n",
    "    train_array = np.array(train_df)\n",
    "    test_array = np.array(test_df)\n",
    "\n",
    "    # Move the labels to {0, ..., L-1}\n",
    "    labels = np.unique(train_array[:, 0])\n",
    "    transform = {}\n",
    "    for i, l in enumerate(labels):\n",
    "        transform[l] = i\n",
    "\n",
    "    train = train_array[:, 1:].astype(np.float64)\n",
    "    train_labels = np.vectorize(transform.get)(train_array[:, 0])\n",
    "    test = test_array[:, 1:].astype(np.float64)\n",
    "    test_labels = np.vectorize(transform.get)(test_array[:, 0])\n",
    "\n",
    "    # Normalization for non-normalized datasets\n",
    "    # To keep the amplitude information, we do not normalize values over\n",
    "    # individual time series, but on the whole dataset\n",
    "    if dataset not in [\n",
    "        'AllGestureWiimoteX',\n",
    "        'AllGestureWiimoteY',\n",
    "        'AllGestureWiimoteZ',\n",
    "        'BME',\n",
    "        'Chinatown',\n",
    "        'Crop',\n",
    "        'EOGHorizontalSignal',\n",
    "        'EOGVerticalSignal',\n",
    "        'Fungi',\n",
    "        'GestureMidAirD1',\n",
    "        'GestureMidAirD2',\n",
    "        'GestureMidAirD3',\n",
    "        'GesturePebbleZ1',\n",
    "        'GesturePebbleZ2',\n",
    "        'GunPointAgeSpan',\n",
    "        'GunPointMaleVersusFemale',\n",
    "        'GunPointOldVersusYoung',\n",
    "        'HouseTwenty',\n",
    "        'InsectEPGRegularTrain',\n",
    "        'InsectEPGSmallTrain',\n",
    "        'MelbournePedestrian',\n",
    "        'PickupGestureWiimoteZ',\n",
    "        'PigAirwayPressure',\n",
    "        'PigArtPressure',\n",
    "        'PigCVP',\n",
    "        'PLAID',\n",
    "        'PowerCons',\n",
    "        'Rock',\n",
    "        'SemgHandGenderCh2',\n",
    "        'SemgHandMovementCh2',\n",
    "        'SemgHandSubjectCh2',\n",
    "        'ShakeGestureWiimoteZ',\n",
    "        'SmoothSubspace',\n",
    "        'UMD'\n",
    "    ]:\n",
    "        return train[..., np.newaxis], train_labels, test[..., np.newaxis], test_labels\n",
    "    \n",
    "    mean = np.nanmean(train)\n",
    "    std = np.nanstd(train)\n",
    "    train = (train - mean) / std\n",
    "    test = (test - mean) / std\n",
    "    return train[..., np.newaxis], train_labels, test[..., np.newaxis], test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = 'classification'\n",
    "train_data, train_labels, test_data, test_labels = load_UCR('ECG200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "    \n",
    "def pad_nan_to_target(array, target_length, axis=0, both_side=False):\n",
    "    assert array.dtype in [np.float16, np.float32, np.float64]\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "    npad = [(0, 0)] * array.ndim\n",
    "    if both_side:\n",
    "        npad[axis] = (pad_size // 2, pad_size - pad_size//2)\n",
    "    else:\n",
    "        npad[axis] = (0, pad_size)\n",
    "    return np.pad(array, pad_width=npad, mode='constant', constant_values=np.nan)\n",
    "\n",
    "def split_with_nan(x, sections, axis=0):\n",
    "    assert x.dtype in [np.float16, np.float32, np.float64]\n",
    "    arrs = np.array_split(x, sections, axis=axis)\n",
    "    target_length = arrs[0].shape[axis]\n",
    "    for i in range(len(arrs)):\n",
    "        arrs[i] = pad_nan_to_target(arrs[i], target_length, axis=axis)\n",
    "    return arrs\n",
    "\n",
    "def centerize_vary_length_series(x):\n",
    "    prefix_zeros = np.argmax(~np.isnan(x).all(axis=-1), axis=1)\n",
    "    suffix_zeros = np.argmax(~np.isnan(x[:, ::-1]).all(axis=-1), axis=1)\n",
    "    offset = (prefix_zeros + suffix_zeros) // 2 - prefix_zeros\n",
    "    rows, column_indices = np.ogrid[:x.shape[0], :x.shape[1]]\n",
    "    offset[offset < 0] += x.shape[1]\n",
    "    column_indices = column_indices - offset[:, np.newaxis]\n",
    "    return x[rows, column_indices]\n",
    "\n",
    "def take_per_row(A, indx, num_elem):\n",
    "    all_indx = indx[:,None] + np.arange(num_elem)\n",
    "    return A[torch.arange(all_indx.shape[0])[:,None], all_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import TSEncoder\n",
    "\n",
    "\n",
    "input_dims=train_data.shape[-1]\n",
    "output_dims=args.repr_dims\n",
    "hidden_dims=64\n",
    "depth=10\n",
    "device='cuda'\n",
    "lr=args.lr\n",
    "batch_size=args.batch_size\n",
    "max_train_length=args.max_train_length\n",
    "temporal_unit=0\n",
    "\n",
    "idx_epochs = 0\n",
    "idx_iters = 0\n",
    "\n",
    "model = TSEncoder(input_dims=input_dims, output_dims=output_dims, hidden_dims=hidden_dims, depth=depth).to(device)\n",
    "net = torch.optim.swa_utils.AveragedModel(model)\n",
    "net.update_parameters(model)\n",
    "\n",
    "n_epochs = None\n",
    "n_iters = None\n",
    "\n",
    "verbose=True\n",
    "\n",
    "\n",
    "assert train_data.ndim == 3\n",
    "\n",
    "if n_iters is None and n_epochs is None:\n",
    "    n_iters = 200 if train_data.size <= 100000 else 600  # default param for n_iters, 600, 3000\n",
    "\n",
    "if max_train_length is not None:  # max_train_length 3000\n",
    "    sections = train_data.shape[1] // max_train_length\n",
    "    if sections >= 2:\n",
    "        train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
    "        \n",
    "temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
    "if temporal_missing[0] or temporal_missing[-1]:\n",
    "    train_data = centerize_vary_length_series(train_data)\n",
    "\n",
    "train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_data).to(torch.float))\n",
    "train_loader = DataLoader(train_dataset, batch_size=min(batch_size, len(train_dataset)), shuffle=True, drop_last=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "while True:\n",
    "    if n_epochs is not None and idx_epochs >= n_epochs:\n",
    "        break\n",
    "    \n",
    "    cum_loss = 0\n",
    "    n_epoch_iters = 0\n",
    "    \n",
    "    interrupted = False\n",
    "    for batch in train_loader:\n",
    "        if n_iters is not None and idx_iters >= n_iters:\n",
    "            interrupted = True\n",
    "            break\n",
    "        \n",
    "        x = batch[0]\n",
    "        if max_train_length is not None and x.size(1) > max_train_length:\n",
    "            window_offset = np.random.randint(x.size(1) - max_train_length + 1)\n",
    "            x = x[:, window_offset : window_offset + max_train_length]\n",
    "        x = x.to(device)\n",
    "        \n",
    "        ts_l = x.size(1)\n",
    "        crop_l = np.random.randint(low=2 ** (temporal_unit + 1), high=ts_l+1)\n",
    "        crop_left = np.random.randint(ts_l - crop_l + 1)\n",
    "        crop_right = crop_left + crop_l\n",
    "        crop_eleft = np.random.randint(crop_left + 1)\n",
    "        crop_eright = np.random.randint(low=crop_right, high=ts_l + 1)\n",
    "        crop_offset = np.random.randint(low=-crop_eleft, high=ts_l - crop_eright + 1, size=x.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out1 = model(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft))\n",
    "        out1 = out1[:, -crop_l:]\n",
    "        \n",
    "        out2 = model(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left))\n",
    "        out2 = out2[:, :crop_l]\n",
    "        \n",
    "        loss = hierarchical_contrastive_loss(\n",
    "            out1,\n",
    "            out2,\n",
    "            temporal_unit=temporal_unit\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.update_parameters(model)\n",
    "            \n",
    "        cum_loss += loss.item()\n",
    "        n_epoch_iters += 1\n",
    "        \n",
    "        idx_iters += 1\n",
    "    \n",
    "    if interrupted:\n",
    "        break\n",
    "    \n",
    "    cum_loss /= n_epoch_iters\n",
    "    loss_log.append(cum_loss)\n",
    "    if verbose:\n",
    "        print(f\"Epoch #{n_epochs}: loss={cum_loss}\")\n",
    "    idx_epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft)\n",
    "temp2 = take_per_row(x, crop_offset + crop_left, crop_eright - crop_left)\n",
    "crop_l, crop_left, crop_right, crop_eleft, crop_eright, crop_offset, temp1.shape, temp2.shape, out1.shape, out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TS2Vec:\n",
    "    '''The TS2Vec model'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims=320,\n",
    "        hidden_dims=64,\n",
    "        depth=10,\n",
    "        device='cuda',\n",
    "        lr=0.001,\n",
    "        batch_size=16,\n",
    "        max_train_length=None,\n",
    "        temporal_unit=0,\n",
    "        after_iter_callback=None,\n",
    "        after_epoch_callback=None\n",
    "    ):\n",
    "        ''' Initialize a TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            input_dims (int): The input dimension. For a univariate time series, this should be set to 1.\n",
    "            output_dims (int): The representation dimension.\n",
    "            hidden_dims (int): The hidden dimension of the encoder.\n",
    "            depth (int): The number of hidden residual blocks in the encoder.\n",
    "            device (int): The gpu used for training and inference.\n",
    "            lr (int): The learning rate.\n",
    "            batch_size (int): The batch size.\n",
    "            max_train_length (Union[int, NoneType]): The maximum allowed sequence length for training. For sequence with a length greater than <max_train_length>, it would be cropped into some sequences, each of which has a length less than <max_train_length>.\n",
    "            temporal_unit (int): The minimum unit to perform temporal contrast. When training on a very long sequence, this param helps to reduce the cost of time and memory.\n",
    "            after_iter_callback (Union[Callable, NoneType]): A callback function that would be called after each iteration.\n",
    "            after_epoch_callback (Union[Callable, NoneType]): A callback function that would be called after each epoch.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.max_train_length = max_train_length\n",
    "        self.temporal_unit = temporal_unit\n",
    "        \n",
    "        self._net = TSEncoder(input_dims=input_dims, output_dims=output_dims, hidden_dims=hidden_dims, depth=depth).to(self.device)\n",
    "        self.net = torch.optim.swa_utils.AveragedModel(self._net)\n",
    "        self.net.update_parameters(self._net)\n",
    "        \n",
    "        self.after_iter_callback = after_iter_callback\n",
    "        self.after_epoch_callback = after_epoch_callback\n",
    "        \n",
    "        self.n_epochs = 0\n",
    "        self.n_iters = 0\n",
    "    \n",
    "    def fit(self, train_data, n_epochs=None, n_iters=None, verbose=False):\n",
    "        ''' Training the TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            train_data (numpy.ndarray): The training data. It should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            n_epochs (Union[int, NoneType]): The number of epochs. When this reaches, the training stops.\n",
    "            n_iters (Union[int, NoneType]): The number of iterations. When this reaches, the training stops. If both n_epochs and n_iters are not specified, a default setting would be used that sets n_iters to 200 for a dataset with size <= 100000, 600 otherwise.\n",
    "            verbose (bool): Whether to print the training loss after each epoch.\n",
    "            \n",
    "        Returns:\n",
    "            loss_log: a list containing the training losses on each epoch.\n",
    "        '''\n",
    "        assert train_data.ndim == 3\n",
    "        \n",
    "        if n_iters is None and n_epochs is None:\n",
    "            n_iters = 600 if train_data.size <= 1000000 else 3000  # default param for n_iters\n",
    "        \n",
    "        # if self.max_train_length is not None:  # max_train_length 3000\n",
    "        #     sections = train_data.shape[1] // self.max_train_length\n",
    "        #     if sections >= 2:\n",
    "        #         train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
    "\n",
    "        temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
    "        if temporal_missing[0] or temporal_missing[-1]:\n",
    "            train_data = centerize_vary_length_series(train_data)\n",
    "                \n",
    "        train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
    "        \n",
    "        train_dataset = TensorDataset(torch.from_numpy(train_data).to(torch.float))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=min(self.batch_size, len(train_dataset)), shuffle=True, drop_last=True)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self._net.parameters(), lr=self.lr)\n",
    "        \n",
    "        loss_log = []\n",
    "        \n",
    "        while True:\n",
    "            if n_epochs is not None and self.n_epochs >= n_epochs:\n",
    "                break\n",
    "            \n",
    "            cum_loss = 0\n",
    "            n_epoch_iters = 0\n",
    "            \n",
    "            interrupted = False\n",
    "            for batch in train_loader:\n",
    "                if n_iters is not None and self.n_iters >= n_iters:\n",
    "                    interrupted = True\n",
    "                    break\n",
    "                \n",
    "                x = batch[0]\n",
    "                if self.max_train_length is not None and x.size(1) > self.max_train_length:\n",
    "                    window_offset = np.random.randint(x.size(1) - self.max_train_length + 1)\n",
    "                    x = x[:, window_offset : window_offset + self.max_train_length]\n",
    "                x = x.to(self.device)\n",
    "                \n",
    "                ts_l = x.size(1)\n",
    "                crop_l = np.random.randint(low=2 ** (self.temporal_unit + 1), high=ts_l+1)\n",
    "                crop_left = np.random.randint(ts_l - crop_l + 1)\n",
    "                crop_right = crop_left + crop_l\n",
    "                crop_eleft = np.random.randint(crop_left + 1)\n",
    "                crop_eright = np.random.randint(low=crop_right, high=ts_l + 1)\n",
    "                crop_offset = np.random.randint(low=-crop_eleft, high=ts_l - crop_eright + 1, size=x.size(0))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                out1 = self._net(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft))\n",
    "                out1 = out1[:, -crop_l:]\n",
    "                \n",
    "                out2 = self._net(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left))\n",
    "                out2 = out2[:, :crop_l]\n",
    "                \n",
    "                loss = hierarchical_contrastive_loss(\n",
    "                    out1,\n",
    "                    out2,\n",
    "                    temporal_unit=self.temporal_unit\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.net.update_parameters(self._net)\n",
    "                    \n",
    "                cum_loss += loss.item()\n",
    "                n_epoch_iters += 1\n",
    "                \n",
    "                self.n_iters += 1\n",
    "                \n",
    "                if self.after_iter_callback is not None:\n",
    "                    self.after_iter_callback(self, loss.item())\n",
    "            \n",
    "            if interrupted:\n",
    "                break\n",
    "            \n",
    "            cum_loss /= n_epoch_iters\n",
    "            loss_log.append(cum_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch #{self.n_epochs}: loss={cum_loss}\")\n",
    "            self.n_epochs += 1\n",
    "            \n",
    "            if self.after_epoch_callback is not None:\n",
    "                self.after_epoch_callback(self, cum_loss)\n",
    "            \n",
    "        return loss_log\n",
    "    \n",
    "    def _eval_with_pooling(self, x, mask=None, slicing=None, encoding_window=None):\n",
    "        out = self.net(x.to(self.device, non_blocking=True), mask)\n",
    "        if encoding_window == 'full_series':\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = out.size(1),\n",
    "            ).transpose(1, 2)\n",
    "            \n",
    "        elif isinstance(encoding_window, int):\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = encoding_window,\n",
    "                stride = 1,\n",
    "                padding = encoding_window // 2\n",
    "            ).transpose(1, 2)\n",
    "            if encoding_window % 2 == 0:\n",
    "                out = out[:, :-1]\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        elif encoding_window == 'multiscale':\n",
    "            p = 0\n",
    "            reprs = []\n",
    "            while (1 << p) + 1 < out.size(1):\n",
    "                t_out = F.max_pool1d(\n",
    "                    out.transpose(1, 2),\n",
    "                    kernel_size = (1 << (p + 1)) + 1,\n",
    "                    stride = 1,\n",
    "                    padding = 1 << p\n",
    "                ).transpose(1, 2)\n",
    "                if slicing is not None:\n",
    "                    t_out = t_out[:, slicing]\n",
    "                reprs.append(t_out)\n",
    "                p += 1\n",
    "            out = torch.cat(reprs, dim=-1)\n",
    "            \n",
    "        else:\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        return out.cpu()\n",
    "    \n",
    "    def encode(self, data, mask=None, encoding_window=None, causal=False, sliding_length=None, sliding_padding=0, batch_size=None):\n",
    "        ''' Compute representations using the model.\n",
    "        \n",
    "        Args:\n",
    "            data (numpy.ndarray): This should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            mask (str): The mask used by encoder can be specified with this parameter. This can be set to 'binomial', 'continuous', 'all_true', 'all_false' or 'mask_last'.\n",
    "            encoding_window (Union[str, int]): When this param is specified, the computed representation would the max pooling over this window. This can be set to 'full_series', 'multiscale' or an integer specifying the pooling kernel size.\n",
    "            causal (bool): When this param is set to True, the future informations would not be encoded into representation of each timestamp.\n",
    "            sliding_length (Union[int, NoneType]): The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "            sliding_padding (int): This param specifies the contextual data length used for inference every sliding windows.\n",
    "            batch_size (Union[int, NoneType]): The batch size used for inference. If not specified, this would be the same batch size as training.\n",
    "            \n",
    "        Returns:\n",
    "            repr: The representations for data.\n",
    "        '''\n",
    "        assert self.net is not None, 'please train or load a net first'\n",
    "        assert data.ndim == 3\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        n_samples, ts_l, _ = data.shape\n",
    "\n",
    "        org_training = self.net.training\n",
    "        self.net.eval()\n",
    "        \n",
    "        dataset = TensorDataset(torch.from_numpy(data).to(torch.float))\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for batch in loader:\n",
    "                x = batch[0]\n",
    "                if sliding_length is not None:\n",
    "                    reprs = []\n",
    "                    if n_samples < batch_size:\n",
    "                        calc_buffer = []\n",
    "                        calc_buffer_l = 0\n",
    "                    for i in range(0, ts_l, sliding_length):\n",
    "                        l = i - sliding_padding\n",
    "                        r = i + sliding_length + (sliding_padding if not causal else 0)\n",
    "                        x_sliding = torch_pad_nan(\n",
    "                            x[:, max(l, 0) : min(r, ts_l)],\n",
    "                            left=-l if l<0 else 0,\n",
    "                            right=r-ts_l if r>ts_l else 0,\n",
    "                            dim=1\n",
    "                        )\n",
    "                        if n_samples < batch_size:\n",
    "                            if calc_buffer_l + n_samples > batch_size:\n",
    "                                out = self._eval_with_pooling(\n",
    "                                    torch.cat(calc_buffer, dim=0),\n",
    "                                    mask,\n",
    "                                    slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                    encoding_window=encoding_window\n",
    "                                )\n",
    "                                reprs += torch.split(out, n_samples)\n",
    "                                calc_buffer = []\n",
    "                                calc_buffer_l = 0\n",
    "                            calc_buffer.append(x_sliding)\n",
    "                            calc_buffer_l += n_samples\n",
    "                        else:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                x_sliding,\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs.append(out)\n",
    "\n",
    "                    if n_samples < batch_size:\n",
    "                        if calc_buffer_l > 0:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                torch.cat(calc_buffer, dim=0),\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs += torch.split(out, n_samples)\n",
    "                            calc_buffer = []\n",
    "                            calc_buffer_l = 0\n",
    "                    \n",
    "                    out = torch.cat(reprs, dim=1)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = F.max_pool1d(\n",
    "                            out.transpose(1, 2).contiguous(),\n",
    "                            kernel_size = out.size(1),\n",
    "                        ).squeeze(1)\n",
    "                else:\n",
    "                    out = self._eval_with_pooling(x, mask, encoding_window=encoding_window)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = out.squeeze(1)\n",
    "                        \n",
    "                output.append(out)\n",
    "                \n",
    "            output = torch.cat(output, dim=0)\n",
    "            \n",
    "        self.net.train(org_training)\n",
    "        return output.numpy()\n",
    "    \n",
    "    def save(self, fn):\n",
    "        ''' Save the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        torch.save(self.net.state_dict(), fn)\n",
    "    \n",
    "    def load(self, fn):\n",
    "        ''' Load the model from a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        state_dict = torch.load(fn, map_location=self.device)\n",
    "        self.net.load_state_dict(state_dict)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
